{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260c3a63",
   "metadata": {},
   "source": [
    "**2\\. Анализ данных (Data Understanding)**\n",
    "\n",
    "Цель шага – понять слабые и сильные стороны предоставленных данных, определить их достаточность, предложить идеи, как  \n",
    "их использовать, и лучше понять процессы заказчика. Для этого мы строим графики, делаем выборки и рассчитываем  \n",
    "статистики.\n",
    "\n",
    "2.1 Сбор данных (Data collection)  \n",
    "Выгружаем необходимые данные (или срез данных если их объем слишком велик) из источников.  \n",
    "Версионируем данные средствами DVC.\n",
    "\n",
    "2.2 Исследование данных (Data exploration)  \n",
    "Исследуем данные, чтобы сформулировать гипотезы относительно того, как эти данные помогут решить задачу. Проверяем  \n",
    "качество данных.\n",
    "\n",
    "Ориентировочный список для проверки данных:\n",
    "\n",
    "1. Загрузить репрезентативную выборку из набора данных  \n",
    "2. Провести предварительный анализ всей выборки.  \n",
    "* определить тип данных в каждом столбце  \n",
    "* Категориальные данные (Номинальные, Порядковые)  \n",
    "* Числовые данные (дискретные, непрерывные, интервальные, отношения)  \n",
    "* При необходимости преобразовать данные к нужным типам  \n",
    "* Проверить на выбросы, отсутствующие значения, невалидные значения(например в системе случился сбой и в поле с именемпопала длина просмотра).\n",
    "\n",
    "(по результатам предварительного анализа сделать визуализацию, обычно это табличка с характеристиками или какой то из  \n",
    "профайлеров)\n",
    "\n",
    "3. На основе предыдущего анализа выполнить очистку данных (обработать выбросы, отсутствующие значения, удалить невалидные значения)  \n",
    "4. Удалить из рассмотрения неинформативные данные. (лишние идентификаторы, служебные поля, поля с очень малым количеством значений)  \n",
    "5. Провести статистический анализ оставшихся данных  \n",
    "   1. рассчитать ключевые статистики для каждого типа данных  \n",
    "   2. построить распределения (тип графика выбрать в зависимости от данных, часто полезно построить гистограмму, но иногда лучше воспользоваться линейным графиком или посмотреть распределение во времени с помощью scaterplot)  \n",
    "6. Провести корреляционный анализ  \n",
    "   1. Для количественных данных нормализовать данные и построить матрицу корреляции Пирсона  \n",
    "   2. сделать выводы на основе матрицы (найти утечки данных, найти важные признаки линейно влияющие на целевой показатель, определить гипотезы по конструированию признаков)  \n",
    "   3. Для количественных и порядковых данных \\- построить матрицу корреляции Спирмена  сделать выводы аналогично предыдущему анализу, только учесть тип данных  \n",
    "   4. Для всех данных построить матрицу корреляции Пфика  \n",
    "   5. Сделать выводы на основе анализа, сделать оценку между всеми типами корреляции(на пересекающихся данных),  \n",
    "   6. попытаться найти объяснения различиям.  \n",
    "   7. Сделать выводы о наличии или отсутствии нелинейных связей.  \n",
    "        \n",
    "7.  Провести обработку данных, на основе выводов полученных в прошлых шагах.  \n",
    "   1. провести дополнительную очистку  \n",
    "   2. выполнить нужный тип энкодинга(если требуется)  \n",
    "   3. Сконструировать новые признаки.\n",
    "\n",
    "8. Построить Графики взаимодействия полученных данных с целевым показателем  \n",
    "   1. Для количественных данных линейные графики на нормализованных данных  \n",
    "   2. для категориальных данных с малым количеством категорий построить ScaterPlot во времени  \n",
    "   3. для категориальных данных с большим количеством показателей построить heatmap во времени(обычно строят, только для ризнаков которые показывают высокие коэффициенты корреляции и потенциально интересны, пример такого графика спектрограмма в анализе звука)  \n",
    "9. Сделать выводы на основе проведенного анализа и учитывая особенности планируемой архитектуры модели.  \n",
    "   1. Какие данные и почему нельзя использовать в модели  \n",
    "   2. какие данные можно использовать без преобразования  \n",
    "   3. какие данные можно использовать выполнив преобразование  \n",
    "   4. какие новые признаки нужно использовать и почему  \n",
    "   5. есть ли смысл использовать один набор признаков или построить разные модели на подмножестве признаков и почему.  \n",
    "   6. Выделить итоговый список необходимых данных  \n",
    "10. Описать ожидания от модели на проанализированных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8da14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client import Configuration\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Загружает переменные из .env\n",
    "\n",
    "def get_lakefs_config():\n",
    "    config = Configuration()\n",
    "    config.host = os.getenv(\"LAKEFS_HOST\")\n",
    "    config.username = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "    config.password = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0551aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client import Configuration\n",
    "from lakefs_client import ApiClient\n",
    "from lakefs_client.api import objects_api\n",
    "\n",
    "def load_dataframe(configuration: Configuration, repository: str, ref: str, path: str):\n",
    "    # Создаем клиента\n",
    "    with ApiClient(configuration) as api_client:\n",
    "        obj_api = objects_api.ObjectsApi(api_client)\n",
    "\n",
    "        # Получаем объект\n",
    "        #repository = \"mfdp-fin-fraud-detection-data\"\n",
    "        #ref = \"main\"  # или другой commit/branch/tag\n",
    "        #path = \"ieee-fraud-detection/sample_submission.csv\"\n",
    "\n",
    "        response = obj_api.get_object(repository, ref, path)\n",
    "        content = response.read()\n",
    "\n",
    "        # Например, если CSV\n",
    "        import pandas as pd\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем объект\n",
    "configuration = get_lakefs_config()\n",
    "repository = \"mfdp-fin-fraud-detection-data\"\n",
    "ref = \"main\"  # или другой commit/branch/tag\n",
    "path_input = \"ieee-fraud-detection/sample_submission.csv\"\n",
    "\n",
    "df = load_dataframe(configuration, repository, ref, path_input)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lakefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a236aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs\n",
    "from lakefs.client import Client\n",
    "import pandas as pd\n",
    "\n",
    "clt = Client(\n",
    "    host=os.getenv(\"LAKEFS_HOST\"),\n",
    "    username=os.getenv(\"LAKEFS_ACCESS_KEY\"),\n",
    "    password=os.getenv(\"LAKEFS_SECRET_KEY\"),\n",
    ")\n",
    "\n",
    "# Укажите имя репозитория и ветки\n",
    "repo_name = \"mfdp-fin-fraud-detection-data\"\n",
    "branch_name = \"main\"\n",
    "file_path = \"ieee-fraud-detection/sample_submission.csv\"\n",
    "\n",
    "# Получите объект файла\n",
    "obj = lakefs.repository(repo_name, client=clt).branch(branch_name).object(path=file_path)\n",
    "\n",
    "# Прочитайте содержимое файла\n",
    "with obj.reader(mode='r') as file:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32136a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read into pandas directly by supplying the lakeFS URI...\n",
    "sample_submission = pd.read_csv(f\"lakefs://mfdp-fin-fraud-detection-data/main/ieee-fraud-detection/sample_submission.csv\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Загружает переменные из .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b3961",
   "metadata": {},
   "source": [
    "# 1. create_lakefs_branch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# Переменные окружения\n",
    "LAKEFS_ENDPOINT = os.getenv(\"LAKEFS_ENDPOINT\")\n",
    "LAKEFS_REPO = os.getenv(\"LAKEFS_REPO\")\n",
    "LAKEFS_BRANCH = os.getenv(\"LAKEFS_BRANCH\", \"processing_branch\")\n",
    "LAKEFS_ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "LAKEFS_SECRET_KEY = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "\n",
    "def create_lakefs_branch():\n",
    "    auth = HTTPBasicAuth(LAKEFS_ACCESS_KEY, LAKEFS_SECRET_KEY)\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    branch_url = f\"{LAKEFS_ENDPOINT}/api/v1/repositories/{LAKEFS_REPO}/branches\"\n",
    "\n",
    "    # Пытаемся создать ветку\n",
    "    resp = requests.post(branch_url, json={\"name\": LAKEFS_BRANCH, \"source\": \"main\"}, auth=auth, headers=headers)\n",
    "\n",
    "    if resp.status_code == 201:\n",
    "        print(f\"✅ Branch '{LAKEFS_BRANCH}' created successfully.\")\n",
    "    elif resp.status_code == 409:\n",
    "        print(f\"⚠️ Branch '{LAKEFS_BRANCH}' already exists.\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to create branch: {resp.status_code}\")\n",
    "        print(resp.text)\n",
    "        return\n",
    "\n",
    "    # Получаем список веток\n",
    "    list_resp = requests.get(branch_url, auth=auth)\n",
    "    print(\"📂 Available branches:\")\n",
    "    for branch in list_resp.json().get(\"results\", []):\n",
    "        print(\" -\", branch['id'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_lakefs_branch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e9c7a",
   "metadata": {},
   "source": [
    "# 2. data_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd0cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client import Configuration\n",
    "from lakefs_client import ApiClient\n",
    "from lakefs_client.api import objects_api\n",
    "\n",
    "def load_dataframe(configuration: Configuration, repository: str, ref: str, path: str):\n",
    "    # Создаем клиента\n",
    "    with ApiClient(configuration) as api_client:\n",
    "        obj_api = objects_api.ObjectsApi(api_client)\n",
    "\n",
    "        response = obj_api.get_object(repository, ref, path)\n",
    "        content = response.read()\n",
    "\n",
    "        # Например, если CSV\n",
    "        import pandas as pd\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47960afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from lakefs_client import Configuration, ApiClient\n",
    "from lakefs_client.api.objects_api import ObjectsApi\n",
    "from lakefs_client.models import ObjectStats\n",
    "from lakefs_client.rest import ApiException\n",
    "\n",
    "def save_dataframe(df: pd.DataFrame, configuration: Configuration, repository: str, ref: str, path: str):\n",
    "    try:\n",
    "        with ApiClient(configuration) as api_client:\n",
    "            obj_api = ObjectsApi(api_client)\n",
    "\n",
    "            buffer = BytesIO()\n",
    "            df.to_csv(buffer, index=False)\n",
    "            buffer.seek(0)\n",
    "\n",
    "            # Добавляем атрибут .name (имитация реального файла)\n",
    "            buffer.name = path.split('/')[-1]\n",
    "\n",
    "            obj_api.upload_object(\n",
    "                repository=repository,\n",
    "                branch=ref,\n",
    "                path=path,\n",
    "                content=buffer\n",
    "            )\n",
    "\n",
    "            print(f\"✅ DataFrame saved to lakeFS at {repository}/{ref}/{path}\")\n",
    "    except ApiException as e:\n",
    "        print(f\"❌ Failed to upload object: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb913727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame):\n",
    "    # Загрузка данных (предположим, что DataFrame называется df)\n",
    "    #df = pd.read_csv('your_data.csv')\n",
    "    df = df_cleaned\n",
    "\n",
    "    # Рассчитываем процент пропущенных значений для каждого столбца\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "\n",
    "    # Фильтруем столбцы, у которых меньше 90% пропусков\n",
    "    df_cleaned = df.loc[:, missing_percentage < 90]\n",
    "\n",
    "    # Разделяем данные на числовые и категориальные столбцы\n",
    "    numerical_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Импутация для числовых столбцов (замена пропусков на медиану)\n",
    "    numerical_imputer = SimpleImputer(strategy='median')\n",
    "    df_cleaned[numerical_cols] = numerical_imputer.fit_transform(df_cleaned[numerical_cols])\n",
    "\n",
    "    # Импутация для категориальных столбцов (замена пропусков на наиболее частое значение)\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df_cleaned[categorical_cols] = categorical_imputer.fit_transform(df_cleaned[categorical_cols])\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3df0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "\n",
    "def encoder_data(df: pd.DataFrame):\n",
    "    # Если в числовых столбцах есть текстовые значения, можно преобразовать их в числовые или удалить.\n",
    "    # Например, если колонка 'category' - категориальная, можно ее закодировать.\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Применяем LabelEncoder для категориальных признаков\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))  # Преобразуем в строки перед кодированием\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8ad8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(df: pd.DataFrame):\n",
    "    # Например, добавим фичу как логарифм числового признака\n",
    "    if 'feature' in df.columns:\n",
    "        df['log_feature'] = df['feature'].apply(lambda x: np.log(x + 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Переменные окружения\n",
    "LAKEFS_ENDPOINT = os.getenv(\"LAKEFS_ENDPOINT\")\n",
    "LAKEFS_REPO = os.getenv(\"LAKEFS_REPO\")\n",
    "LAKEFS_BRANCH = os.getenv(\"LAKEFS_BRANCH\", \"processing_branch\")\n",
    "LAKEFS_ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "LAKEFS_SECRET_KEY = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "\n",
    "path_train_transaction_input = \"ieee-fraud-detection/train_transaction.csv\"\n",
    "path_train_identity_input = \"ieee-fraud-detection/train_identity.csv\"\n",
    "\n",
    "path_output = \"ieee-fraud-detection/clean_merge_dataset.csv\"\n",
    "\n",
    "\n",
    "def get_lakefs_config():\n",
    "    config = Configuration()\n",
    "    config.host = os.getenv(\"LAKEFS_HOST\")\n",
    "    config.username = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "    config.password = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    # Чтение данных из CSV\n",
    "    #df = pd.read_csv(INPUT_URI)\n",
    "    configuration = get_lakefs_config()\n",
    "    df_train_transaction = load_dataframe(configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_train_transaction_input)\n",
    "    print(f\"✅ DataFrame transactions loaded frome lakeFS at {LAKEFS_REPO}/{LAKEFS_BRANCH}/{path_train_transaction_input}\")\n",
    "\n",
    "    df_train_identity = load_dataframe(configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_train_identity_input)\n",
    "    print(f\"✅ DataFrame identity loaded frome lakeFS at {LAKEFS_REPO}/{LAKEFS_BRANCH}/{path_train_identity_input}\")\n",
    "    \n",
    "    # Объединение по TransactionID\n",
    "    df = df_train_transaction.merge(df_train_identity, on=\"TransactionID\", how=\"left\")\n",
    "    print(f\"✅ DataFrames transactions and identity merged\")\n",
    "\n",
    "    # Очистка данных (удаляем пропуски)\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Преобразование категориальных признаков (если есть)\n",
    "    df = encoder_data(df)\n",
    "    \n",
    "    # Генерация фичей (если нужно)\n",
    "    df = impute_data(df)\n",
    "    \n",
    "    # Сохранение обработанных данных\n",
    "    save_dataframe(df, configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_output)\n",
    "\n",
    "    print(f\"Processed data saved to {path_output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcccc8",
   "metadata": {},
   "source": [
    "# 3. validate_processed_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed86938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client import Configuration\n",
    "from lakefs_client import ApiClient\n",
    "from lakefs_client.api import objects_api\n",
    "\n",
    "def load_dataframe(configuration: Configuration, repository: str, ref: str, path: str):\n",
    "    # Создаем клиента\n",
    "    with ApiClient(configuration) as api_client:\n",
    "        obj_api = objects_api.ObjectsApi(api_client)\n",
    "\n",
    "        response = obj_api.get_object(repository, ref, path)\n",
    "        content = response.read()\n",
    "\n",
    "        # Например, если CSV\n",
    "        import pandas as pd\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in data\n",
      "Data types: \n",
      "TransactionID      int64\n",
      "isFraud          float64\n",
      "dtype: object\n",
      "Data statistics: \n",
      "       TransactionID   isFraud\n",
      "count   5.066910e+05  506691.0\n",
      "mean    3.916894e+06       0.5\n",
      "std     1.462692e+05       0.0\n",
      "min     3.663549e+06       0.5\n",
      "25%     3.790222e+06       0.5\n",
      "50%     3.916894e+06       0.5\n",
      "75%     4.043566e+06       0.5\n",
      "max     4.170239e+06       0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_input = \"ieee-fraud-detection/clean_merge_dataset.csv\"\n",
    "\n",
    "def validate_data():\n",
    "    # Загрузка обработанных данных\n",
    "    configuration = get_lakefs_config()\n",
    "    df = load_dataframe(configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_input)\n",
    "    \n",
    "    # Проверка на пропуски\n",
    "    if df.isnull().sum().any():\n",
    "        print(\"Warning: Data contains missing values\")\n",
    "    else:\n",
    "        print(\"No missing values in data\")\n",
    "    \n",
    "    # Проверка типов данных\n",
    "    print(f\"Data types: \\n{df.dtypes}\")\n",
    "    \n",
    "    # Вывод статистики\n",
    "    print(f\"Data statistics: \\n{df.describe()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    validate_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf31dc",
   "metadata": {},
   "source": [
    "## 4. commit_preprocessed_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d539a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Commit completed. ID: d16d6c4809525b8a12e4700fcbc2e51d3578407b6ab4f49d207ce7ffdd0eda2e\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client import Configuration, ApiClient\n",
    "from lakefs_client.api.commits_api import CommitsApi\n",
    "from lakefs_client.models import CommitCreation\n",
    "\n",
    "import os\n",
    "\n",
    "# Переменные окружения\n",
    "LAKEFS_REPO = os.getenv(\"LAKEFS_REPO\")\n",
    "LAKEFS_BRANCH = os.getenv(\"LAKEFS_BRANCH\", \"processing_branch\")\n",
    "LAKEFS_ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY\")\n",
    "LAKEFS_SECRET_KEY = os.getenv(\"LAKEFS_SECRET_KEY\")\n",
    "LAKEFS_HOST = os.getenv(\"LAKEFS_HOST\")\n",
    "\n",
    "\n",
    "def get_lakefs_config():\n",
    "    config = Configuration()\n",
    "    config.host = LAKEFS_HOST\n",
    "    config.username = LAKEFS_ACCESS_KEY\n",
    "    config.password = LAKEFS_SECRET_KEY\n",
    "    return config\n",
    "\n",
    "\n",
    "def commit_processed_data():\n",
    "    configuration = get_lakefs_config()\n",
    "    \n",
    "    with ApiClient(configuration) as api_client:\n",
    "        commit_api = CommitsApi(api_client)\n",
    "\n",
    "        commit = CommitCreation(\n",
    "            message=\"💾 Added cleaned sample_submission.csv after preprocessing\"\n",
    "        )\n",
    "\n",
    "        response = commit_api.commit(\n",
    "            repository=LAKEFS_REPO,\n",
    "            branch=LAKEFS_BRANCH,\n",
    "            commit_creation=commit\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Commit completed. ID: {response.id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    commit_processed_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf4605",
   "metadata": {},
   "source": [
    "## 5. train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730e07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client import Configuration\n",
    "from lakefs_client import ApiClient\n",
    "from lakefs_client.api import objects_api\n",
    "\n",
    "def load_dataframe(configuration: Configuration, repository: str, ref: str, path: str):\n",
    "    # Создаем клиента\n",
    "    with ApiClient(configuration) as api_client:\n",
    "        obj_api = objects_api.ObjectsApi(api_client)\n",
    "\n",
    "        response = obj_api.get_object(repository, ref, path)\n",
    "        content = response.read()\n",
    "\n",
    "        # Например, если CSV\n",
    "        import pandas as pd\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dbf5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     mlflow.end_run()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m configuration = get_lakefs_config()\n\u001b[32m     16\u001b[39m df = load_dataframe(configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_input)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m y = df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Инициализация MLflow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Karpov.Courses/mfdp-fin-fraud-detection/.venv/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Karpov.Courses/mfdp-fin-fraud-detection/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Karpov.Courses/mfdp-fin-fraud-detection/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Karpov.Courses/mfdp-fin-fraud-detection/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "#OUTPUT_URI = os.getenv(\"OUTPUT_URI\")\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "\n",
    "path_input = \"ieee-fraud-detection/clean_merge_dataset.csv\"\n",
    "\n",
    "def train_model():\n",
    "    # Загрузка обработанных данных\n",
    "    configuration = get_lakefs_config()\n",
    "    df = load_dataframe(configuration, LAKEFS_REPO, LAKEFS_BRANCH, path_input)\n",
    "    \n",
    "    X = df.drop(\"target\", axis=1)\n",
    "    y = df[\"target\"]\n",
    "    \n",
    "    # Инициализация MLflow\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Обучение модели\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Предсказания\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "\n",
    "    # Логирование метрик и модели\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"auc\", auc)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Завершаем запуск\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ee929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
